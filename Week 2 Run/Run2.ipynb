{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6mxiqXAjFv0"
      },
      "source": [
        "from random import randint\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import svm\n",
        "from nltk import tokenize\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data = pd.read_csv(\"sample.csv\", delimiter='\\t', encoding='utf-8')\n",
        "\n",
        "# Data Preparation\n",
        "\n",
        "# Removing id and title\n",
        "\n",
        "data.drop([\"title\"], axis=1, inplace=True)\n",
        "data.drop([\"public_id\"], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTApiylzjPEp"
      },
      "source": [
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "\n",
        "def punctuation_removal(text):\n",
        "    all_list = [char for char in text if char not in string.punctuation]\n",
        "    clean_str = ''.join(all_list)\n",
        "    return clean_str\n",
        "\n",
        "\n",
        "data['text'] = data['text'].apply(punctuation_removal)\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
        "\n",
        "\n",
        "def line_removal(text):\n",
        "    all_list = [char for char in text if char not in \"-–—_\"]\n",
        "    clean_str = ''.join(all_list)\n",
        "    return clean_str\n",
        "\n",
        "\n",
        "data['text'] = data['text'].apply(line_removal)\n",
        "\n",
        "data['text'] = [word_tokenize(entry) for entry in data['text']]\n",
        "\n",
        "tag_map = defaultdict(lambda: wn.NOUN)\n",
        "tag_map['ADJ'] = wn.ADJ\n",
        "tag_map['VERB'] = wn.VERB\n",
        "tag_map['ADV'] = wn.ADV\n",
        "\n",
        "for index, entry in enumerate(data['text']):\n",
        "    final_words = \"\"\n",
        "    word_lemmatized = WordNetLemmatizer()\n",
        "\n",
        "    for word, tag in pos_tag(entry):\n",
        "        if word not in stopwords.words('english') and word.isalpha():\n",
        "            final_word = word_lemmatized.lemmatize(word, tag_map[tag[0]])\n",
        "            final_words += final_word + \" \"\n",
        "\n",
        "    data.loc[index, 'text'] = final_words\n",
        "\n",
        "print(data['text'])\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "\n",
        "encoded_docs_binary = tokenizer.texts_to_matrix(data['text'], mode=\"binary\")\n",
        "# print(encoded_docs_binary)\n",
        "\n",
        "encoded_docs_count = tokenizer.texts_to_matrix(data['text'], mode=\"count\")\n",
        "# print(encoded_docs_count)\n",
        "\n",
        "encoded_docs_freq = tokenizer.texts_to_matrix(data['text'], mode=\"freq\")\n",
        "# print(encoded_docs_freq)\n",
        "\n",
        "encoded_docs_tfidf = tokenizer.texts_to_matrix(data['text'], mode=\"tfidf\")\n",
        "# print(encoded_docs_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhyseRqI41dO"
      },
      "source": [
        "for i in range(len(data['our rating'])):\n",
        "    if data['our rating'][i] == \"FALSE\":\n",
        "        data['our rating'][i] = 0\n",
        "    elif data['our rating'][i] == \"TRUE\":\n",
        "        data['our rating'][i] = 1\n",
        "    else:\n",
        "        data['our rating'][i] = 2\n",
        "\n",
        "data['our rating'] = data['our rating'].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4yDsV2b46Xb"
      },
      "source": [
        "for i in range(len(data['our rating'])):\n",
        "    if data['our rating'][i] == \"FALSE\":\n",
        "        data['our rating'][i] = 0\n",
        "    elif data['our rating'][i] == \"TRUE\":\n",
        "        data['our rating'][i] = 1\n",
        "    else:\n",
        "        data['our rating'][i] = 0\n",
        "\n",
        "data['our rating'] = data['our rating'].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWJqPN22zx2a"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(encoded_docs_binary, data['our rating'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDPLRzzKzz2b"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(encoded_docs_count, data['our rating'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ia3zgYlz0Wo"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(encoded_docs_freq, data['our rating'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTxkW6pkz02z"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(encoded_docs_tfidf, data['our rating'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-r89yW_kBJj"
      },
      "source": [
        "DT_params = RandomizedSearchCV(estimator=DecisionTreeClassifier(), param_distributions={\n",
        "    \"max_depth\": [1, 2, 3],\n",
        "    \"min_samples_split\": [2, 3, 4, 5]\n",
        "}, n_iter=12, n_jobs=-1)\n",
        "\n",
        "result = DT_params.fit(x_train, y_train)\n",
        "result2 = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "my_decision_tree = DecisionTreeClassifier()\n",
        "\n",
        "my_decision_tree.fit(x_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUM19qaNkY5t"
      },
      "source": [
        "result2['params']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp3LQv3hkj_P"
      },
      "source": [
        "my_decision_tree = DecisionTreeClassifier(max_depth = 2, min_samples_split = 2)\n",
        "my_decision_tree.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_test, my_decision_tree.predict(x_test), zero_division=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKwdOuHuoQKG"
      },
      "source": [
        "SVM_params = RandomizedSearchCV(estimator=svm.SVC(probability=True), param_distributions={\n",
        "    \"C\": [0.1, 0.2, 0.3, 0.4, 0.5, 1, 1.5, 2],\n",
        "    \"kernel\": [\"linear\", \"poly\", \"rbf\"]\n",
        "}, n_iter=24, n_jobs=-1)\n",
        "\n",
        "result3 = SVM_params.fit(x_train, y_train)\n",
        "result4 = pd.DataFrame(result3.cv_results_).loc[[result3.best_index_]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZLJe_LuqFJu"
      },
      "source": [
        "result4[\"params\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNSiLnRfwEKr"
      },
      "source": [
        "my_svm = svm.SVC(probability = True, C = 0.1, kernel = \"linear\")\n",
        "my_svm.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_test, my_svm.predict(x_test), zero_division=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNKd8P1RwOHg"
      },
      "source": [
        "my_nb = ComplementNB()\n",
        "my_nb.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_test, my_nb.predict(x_test), zero_division=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scw6IQPOx-Wc"
      },
      "source": [
        "KNN_params = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions={\n",
        "    \"leaf_size\": list(range(1, 50)),\n",
        "    \"n_neighbors\": list(range(1, 30)),\n",
        "    \"p\": [1, 2]\n",
        "}, n_iter=50, n_jobs=-1)\n",
        "\n",
        "result5 = KNN_params.fit(x_train, y_train)\n",
        "result6 = pd.DataFrame(result5.cv_results_).loc[[result5.best_index_]]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PER_sq0RyxMN"
      },
      "source": [
        "result6[\"params\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYpkSUknzRHS"
      },
      "source": [
        "my_knn = KNeighborsClassifier(p = 2, n_neighbors = 12, leaf_size = 40)\n",
        "my_knn.fit(x_train, y_train)\n",
        "\n",
        "print(classification_report(y_test, my_knn.predict(x_test), zero_division=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fICs_gkk4t5l"
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "a, b, c = roc_curve(y_test, my_decision_tree.predict_proba(x_test)[:,1])\n",
        "a1, b1, c1 = roc_curve(y_test, my_nb.predict_proba(x_test)[:,1])\n",
        "a2, b2, c2 = roc_curve(y_test, my_knn.predict_proba(x_test)[:,1])\n",
        "\n",
        "plt.plot(a, b)\n",
        "plt.plot(a1, b1)\n",
        "plt.plot(a2, b2)\n",
        "\n",
        "plt.legend([\"DT\", \"NB\", \"KNN\"])\n",
        "plt.xlabel(\"false-positive rate\")\n",
        "plt.ylabel(\"true-positive rate\")\n",
        "plt.show()\n",
        "\n",
        "print(\"AUC for DT: %f\" % roc_auc_score(y_test, my_decision_tree.predict_proba(x_test)[:,1]))\n",
        "print(\"AUC for NB: %f\" % roc_auc_score(y_test, my_nb.predict_proba(x_test)[:,1]))\n",
        "print(\"AUC for KNN: %f\" % roc_auc_score(y_test, my_knn.predict_proba(x_test)[:,1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}